{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2nprS8N_oo0"
      },
      "source": [
        "# Decoding of Next-Token Predictions\n",
        "\n",
        "This note book is designed to reimplement Pathscope **section 1:Next-Token Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "id": "uJanliup_oo1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "id": "64jxTToh_oo1",
        "outputId": "8fd0d76f-04ba-4d89-e706-b53527c88d26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e7c79a1f73ba>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Should return True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Should return the number of available GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Should return the name of the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \"\"\"\n\u001b[0;32m--> 523\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())  # Should return True\n",
        "print(torch.cuda.device_count())  # Should return the number of available GPUs\n",
        "print(torch.cuda.get_device_name(0))  # Should return the name of the GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets"
      ],
      "metadata": {
        "id": "e875zyh6BQSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "EeB8w9uL_oo1"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM,DataCollatorWithPadding\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.amp import autocast\n",
        "import os\n",
        "import math\n",
        "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "# import dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdEqA1mWHV7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "1-T3prya_oo2"
      },
      "outputs": [],
      "source": [
        "model_dict = [\"Qwen/Qwen2-0.5B\",\"Qwen/Qwen2.5-0.5B-Instruct\",\"Qwen/Qwen2-1.5B\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "D-fRrVUl_oo2"
      },
      "outputs": [],
      "source": [
        "model_num = 0\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dict[model_num])\n",
        "model = AutoModelForCausalLM.from_pretrained(model_dict[model_num]).to(device)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "hzhqws_q_oo2"
      },
      "outputs": [],
      "source": [
        "# model(tokenizer('how are you doing today?'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4LIU5puY_oo2"
      },
      "outputs": [],
      "source": [
        "print(model.__class__.__module__)\n",
        "print(model.config.model_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-CWX85Ny_oo2"
      },
      "outputs": [],
      "source": [
        "type(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "eNbjX0No_oo2"
      },
      "outputs": [],
      "source": [
        "type(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "CVav0bHF_oo2"
      },
      "outputs": [],
      "source": [
        "# from transformers import Qwen2ForCausalLM\n",
        "# model2 = Qwen2ForCausalLM.from_pretrained('Qwen/Qwen2-0.5B')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "3VPBUZUe_oo2"
      },
      "outputs": [],
      "source": [
        "# print(model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "40M1iupD_oo3"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeW0LGuQ_oo3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "c-gdVbbc_oo3"
      },
      "outputs": [],
      "source": [
        "# define a function to get hidden states of each layer of the model\n",
        "\n",
        "def get_hidden_state(model, input_ids):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids.to(device), output_hidden_states=True)\n",
        "    return outputs.hidden_states[1:]  #Size([batch_size, num_layers, sequence_length, hidden_size])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "3IQg-k_k_oo3"
      },
      "outputs": [],
      "source": [
        "# load the Pile data \"https://huggingface.co/datasets/EleutherAI/pile\" from hugging face, unfortunately it is not available\n",
        "# use the following uncoprighted pile dataset instead\n",
        "# https://huggingface.co/datasets/monology/pile-uncopyrighted\n",
        "\n",
        "dataPile= load_dataset(\"monology/pile-uncopyrighted\",split='train',streaming=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "BLKw80V3_oo3"
      },
      "outputs": [],
      "source": [
        "dataPile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "LwVlP1sL_oo3"
      },
      "outputs": [],
      "source": [
        "# shuffle the dataset before spliting them\n",
        "# reference resource \"https://huggingface.co/docs/datasets/v3.3.2/stream\"\n",
        "\n",
        "dataPile = dataPile.shuffle(seed=50)\n",
        "training_data = dataPile.take(10000)\n",
        "# train : val = 5 : 1\n",
        "validation_data = dataPile.skip(10000).take(2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "QdgyFD9q_oo3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "list(validation_data)[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "R3kdTtjs_oo3"
      },
      "outputs": [],
      "source": [
        "text = \"1. Field of the Invention\\nThe present invention relates to toothbrushes and, in particular, to a toothbrush having a hollow handle defining a paste-holding cavity wherein toothpaste is forcibly dispensed therefrom\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "BC-usxHU_oo3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "tokenizer([text,'hee asdf po afsdf '], return_tensors=\"pt\",max_length=21, truncation=True, padding=\"max_length\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "nwMjqfoi_oo3"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "\n",
        "def preprocess(example):\n",
        "\n",
        "    text = example['text']\n",
        "    inputs = tokenizer(text, return_tensors='pt', max_length=50, truncation=True) # dict_keys(['input_ids', 'attention_mask'])\n",
        "    inputs_ids = inputs['input_ids'][0] # becasue the batch size is 1\n",
        "\n",
        "    if len(inputs_ids) > 5:\n",
        "        trim_len = random.randint(5,  len(inputs_ids) -1)   # to introduce randomness in the training data\n",
        "        inputs_ids = inputs_ids[:trim_len]\n",
        "\n",
        "    return {'input_ids': inputs_ids, 'attention_mask': inputs['attention_mask'][0][:len(inputs_ids)]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "mD5IelfK_oo3"
      },
      "outputs": [],
      "source": [
        "training_data = training_data.map(preprocess,remove_columns=['text','meta'])\n",
        "validation_data = validation_data.map(preprocess,remove_columns=['text','meta'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "cPYrMPex_oo3"
      },
      "outputs": [],
      "source": [
        "next(iter(training_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Ckl6H_9r_oo3"
      },
      "outputs": [],
      "source": [
        "# design a collate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "pR3PO8M6_oo3"
      },
      "outputs": [],
      "source": [
        "# we can change the attentino mask to focus on the token of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ZZoWDAg4_oo3"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BthK8ISp_oo3"
      },
      "outputs": [],
      "source": [
        "# PyTorch\n",
        "torch.cuda.memory_summary()\n",
        "\n",
        "# Or more detailed\n",
        "torch.cuda.memory_allocated()\n",
        "torch.cuda.memory_reserved()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "N9eIBcNC_oo3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1D2cQi4mU0rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPfUxPmj_oo3"
      },
      "source": [
        "## 1. Logit lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "tags": [],
        "id": "9IsLymdH_oo3",
        "outputId": "eba5f28c-4ee0-43b4-8cc7-63c45ef11627",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "logit_lens_exp: 100%|██████████| 63/63 [00:52<00:00,  1.20it/s]\n"
          ]
        }
      ],
      "source": [
        "# each_layer_logit_lens = {l : [] for l in range(len(24))}\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "each_layer_logit_lens_Precision = defaultdict(list) # key = int:layer , value = list[]\n",
        "each_layer_logit_lens_Surprisal = defaultdict(list)\n",
        "\n",
        "def logit_len_for_test(hidden_states,refernce_logits):\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "def logit_len(hidden_states,mask,refernce_logits):\n",
        "    \"\"\"\n",
        "\n",
        "    hidden_states: Tuple[Size([batch_size, sequence_length, hidden_size])]: the hidden states of each layer of the model\n",
        "\n",
        "    mask: Size([batch_size, sequence_length])\n",
        "\n",
        "    refernce_logits: Size([batch_size, sequence_length, vocab_size]): this will be converted to Size([batch_size, vocab_size])\n",
        "\n",
        "    \"\"\"\n",
        "    W_U = model.lm_head.weight\n",
        "\n",
        "    # using attention mask to get the last token of the sequence\n",
        "\n",
        "    num_layer  = len(hidden_states)\n",
        "    batch_size, sequence_length, hidden_size = hidden_states[0].shape\n",
        "\n",
        "    last_token_pos = torch.sum(mask,dim=1) # Size([batch_size])\n",
        "\n",
        "    reference_logits = refernce_logits[torch.arange(len(mask)),last_token_pos -1 ,:] # Size([batch_size, vocab_size])\n",
        "    reference_logits_max_token = torch.argmax(reference_logits,dim = -1)\n",
        "\n",
        "    def suprisla(reference_logist, logist):\n",
        "        predicted_tokens = torch.argmax(logits, dim=-1)  # Shape: (batch_size,)\n",
        "        reference_probs = torch.nn.functional.softmax(reference_logits, dim=-1)\n",
        "        predicted_token_probs = reference_probs[torch.arange(batch_size), predicted_tokens]\n",
        "        surprisal = -torch.log(predicted_token_probs + 1e-9)\n",
        "\n",
        "        return torch.sum(surprisal)\n",
        "\n",
        "\n",
        "\n",
        "    for idx, each_layer in enumerate(hidden_states):\n",
        "        layer_hidden = each_layer[torch.arange(batch_size),last_token_pos-1] # Size([batch_size, hidden_size])\n",
        "        # layer_hidden = layer_hidden[torch.arange(batch_size),:] # Size([batch_size, hidden_size])\n",
        "        logits = torch.einsum('bd,vd->bv', layer_hidden, W_U)  # Size([batch_size, vocab_size])\n",
        "        logits_max_token = torch.argmax(logits,dim = -1)\n",
        "\n",
        "        identical_samples = torch.sum(logits_max_token == reference_logits_max_token)\n",
        "        each_layer_logit_lens_Precision[idx].append(identical_samples)\n",
        "        each_layer_logit_lens_Surprisal[idx].append(suprisla(reference_logits,logits))\n",
        "        # print(identical_samples)\n",
        "\n",
        "\n",
        "    return\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "i = 0\n",
        "for batch in tqdm(DataLoader(validation_data, batch_size=batch_size, collate_fn=data_collator),total = math.ceil(2000/batch_size),desc = f\"logit_lens_exp\"):\n",
        "    # print(batch)k\n",
        "    batch.to(device)\n",
        "    with torch.no_grad():\n",
        "        model_outputs = model(**batch,output_hidden_states=True)\n",
        "        hidden_states = model_outputs.hidden_states[1:]  # exclude the embedding layer\n",
        "        reference_logits = model_outputs.logits\n",
        "        mask = batch['attention_mask']\n",
        "\n",
        "    # reference_logits = reference_logits[torch.arange(len(mask)),torch.sum(mask,dim=1)] # Size([batch_size, vocab_size])\n",
        "    Compare_logit = logit_len(hidden_states,mask,reference_logits)\n",
        "    # i += 1\n",
        "    # print('one batch done')\n",
        "\n",
        "    # if i >= 2:\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "nxwqJ51F_oo4"
      },
      "outputs": [],
      "source": [
        "# Clear GPU cache\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "4e4ml62J_oo4"
      },
      "outputs": [],
      "source": [
        "# print(# PyTorch\n",
        "# torch.cuda.memory_summary())\n",
        "\n",
        "# # Or more detailed\n",
        "# print(torch.cuda.memory_allocated())\n",
        "# print(torch.cuda.memory_reserved())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH0Jzcvm_oo4"
      },
      "source": [
        "#### Metric\n",
        "1. Precision@1\n",
        "2. Suprisal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "tags": [],
        "id": "AVSUPWdW_oo4"
      },
      "outputs": [],
      "source": [
        "def precision(a):\n",
        "    \"\"\"\n",
        "    a : dict[layer] =  list[]\n",
        "    \"\"\"\n",
        "\n",
        "    for k, v in a.items():\n",
        "        a[k] = torch.stack(v,dim = 0).sum() / 2000\n",
        "\n",
        "    return a\n",
        "\n",
        "\n",
        "def surprisal(a):\n",
        "\n",
        "    for k,v in a.items():\n",
        "        a[k] = torch.stack(v,dim = 0).sum()/2000\n",
        "\n",
        "    return a\n",
        "\n",
        "logit_len_p = precision(each_layer_logit_lens_Precision)\n",
        "logit_len_s = surprisal(each_layer_logit_lens_Surprisal)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(logit_len_p,f\"/content/drive/MyDrive/Patchscope-Reimplementation-main/Decoding_of_next_token_Prediction/logit_len_P_{model_dict[model_num].split('/')[-1]}.pt\")\n",
        "torch.save(logit_len_s,f\"/content/drive/MyDrive/Patchscope-Reimplementation-main/Decoding_of_next_token_Prediction/logit_len_S_{model_dict[model_num].split('/')[-1]}.pt\")"
      ],
      "metadata": {
        "id": "aWoURqtbJykf"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YBKWmat3KJHz",
        "outputId": "5ce7c510-716a-4daa-cdc0-0d54539a119e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logit_len_p"
      ],
      "metadata": {
        "id": "CJinVEfECmM-",
        "outputId": "4f243a59-8ddb-4d65-b666-7601681549e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {0: tensor(0.0040, device='cuda:0'),\n",
              "             1: tensor(0.0015, device='cuda:0'),\n",
              "             2: tensor(0.0005, device='cuda:0'),\n",
              "             3: tensor(0.0010, device='cuda:0'),\n",
              "             4: tensor(0.0035, device='cuda:0'),\n",
              "             5: tensor(0.0015, device='cuda:0'),\n",
              "             6: tensor(0.0020, device='cuda:0'),\n",
              "             7: tensor(0.0025, device='cuda:0'),\n",
              "             8: tensor(0.0045, device='cuda:0'),\n",
              "             9: tensor(0.0045, device='cuda:0'),\n",
              "             10: tensor(0.0040, device='cuda:0'),\n",
              "             11: tensor(0.0030, device='cuda:0'),\n",
              "             12: tensor(0.0090, device='cuda:0'),\n",
              "             13: tensor(0.0120, device='cuda:0'),\n",
              "             14: tensor(0.0130, device='cuda:0'),\n",
              "             15: tensor(0.0170, device='cuda:0'),\n",
              "             16: tensor(0.0620, device='cuda:0'),\n",
              "             17: tensor(0.0650, device='cuda:0'),\n",
              "             18: tensor(0.0630, device='cuda:0'),\n",
              "             19: tensor(0.1055, device='cuda:0'),\n",
              "             20: tensor(0.1255, device='cuda:0'),\n",
              "             21: tensor(0.1825, device='cuda:0'),\n",
              "             22: tensor(0.3195, device='cuda:0'),\n",
              "             23: tensor(1., device='cuda:0')})"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "tags": [],
        "id": "UKHUlogp_oo_",
        "outputId": "275d563d-3863-4a74-cf95-4ba957b28e0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 48, 151936])\n",
            "torch.Size([16, 48, 896])\n"
          ]
        }
      ],
      "source": [
        "print(model_outputs['logits'].shape)\n",
        "print(model_outputs['hidden_states'][4].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "tags": [],
        "id": "H9tksH9f_oo_",
        "outputId": "29442f1a-8aab-49b0-8abe-498fe3b6e8fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "len(model_outputs['hidden_states'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0c5SgpZ_oo_"
      },
      "source": [
        "## 2. Tuned Lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "tags": [],
        "id": "wGhtLgIw_oo_",
        "outputId": "fadb3f0b-c258-427a-8f2d-7117c5a58baa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "affine map training data: 100%|██████████| 313/313 [03:39<00:00,  1.43it/s]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "config = model.config\n",
        "num_layers = config.num_hidden_layers\n",
        "\n",
        "X_train = {l : [] for l in range(num_layers)}\n",
        "Y_train = []  # finaly layer logits\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "def get_last_token_hidden_states(hidden_states,mask) -> None:\n",
        "    \"\"\"\n",
        "    hiddenstates: Tuple[Size([batch_size, sequence_length, hidden_size])]: the hidden states of each layer of the model\n",
        "    \"\"\"\n",
        "\n",
        "    last_token_pos = torch.sum(mask,dim=1) # Size([batch_size])\n",
        "\n",
        "    for i, layer in enumerate(hidden_states):\n",
        "        X_train[i].append(layer[torch.arange(len(mask)),last_token_pos -1 ,:].cpu().numpy())   # Size([batch_size, hidden_size])\n",
        "\n",
        "    Y_train.append(hidden_states[-1][torch.arange(len(mask)),last_token_pos -1 ,:].cpu().numpy())   # Size([batch_size, hidden_size]))\n",
        "\n",
        "# to get the hidden states of each sequence\n",
        "for batch in tqdm(DataLoader(training_data, batch_size=batch_size, collate_fn=data_collator),total = math.ceil(10000 / batch_size) ,desc = \"affine map training data\"):\n",
        "    batch.to(device)\n",
        "    with torch.no_grad():\n",
        "        model_outputs = model(**batch,output_hidden_states=True)\n",
        "    # batch.to('cpu')\n",
        "    hidden_states = model_outputs.hidden_states[1:] # exclude the embedding layer\n",
        "    mask = batch['attention_mask'].cpu()\n",
        "    reference_logits = model_outputs.logits\n",
        "\n",
        "    get_last_token_hidden_states(hidden_states,mask)\n",
        "\n",
        "#     del batch, model_outputs\n",
        "#     torch.cuda.empty_cache()  # Clear GPU cache\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "tags": [],
        "id": "HDqLIkX5_oo_",
        "outputId": "b4482a85-143c-417c-c7a7-01dc1565ca9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 896)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "X_train[0][-1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "tags": [],
        "id": "GwtaUKG__oo_",
        "outputId": "ac024629-1923-48b6-eddb-e63956b5337a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "313"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "len(Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "sOiLjyc5_oo_"
      },
      "outputs": [],
      "source": [
        "# for each layer,train a affing transformation to predict the final layer logits\n",
        "\n",
        "Y_train = np.concatenate(Y_train,axis=0) # Size([num_samples, hidden_size])\n",
        "\n",
        "affine_map_house = []\n",
        "\n",
        "for i in range(num_layers):\n",
        "    affine_map = LinearRegression(fit_intercept=True)\n",
        "    X_train[i] = np.concatenate(X_train[i],axis=0) # Size([num_samples, hidden_size])\n",
        "    affine_map.fit(X_train[i],Y_train)\n",
        "    affine_map_house.append(affine_map)\n",
        "    # break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Q3ZaHXN3_oo_",
        "outputId": "f45492cd-b5de-4dad-87af-8aae276cd1e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tuned_len_exp: 100%|██████████| 63/63 [00:59<00:00,  1.05it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# do the same thing as logit_len but with the affine transformation bewteen the hidden states of each layer and the final layer logits\n",
        "each_layer_tuned_lens_Precision = defaultdict(list)\n",
        "each_layer_tuned_lens_Surprisal = defaultdict(list)\n",
        "\n",
        "def tuned_len(hidden_states,mask,refernce_logits,model = model):\n",
        "    \"\"\"\n",
        "\n",
        "    hidden_states: Tuple[Size([batch_size, sequence_length, hidden_size])]: the hidden states of each layer of the model\n",
        "\n",
        "    mask: Size([batch_size, sequence_length])\n",
        "\n",
        "    refernce_logits: Size([batch_size, sequence_length, vocab_size]): this will be converted to Size([batch_size, vocab_size])\n",
        "\n",
        "    \"\"\"\n",
        "    W_U = model.lm_head.weight\n",
        "\n",
        "    # using attention mask to get the last token of the sequence\n",
        "\n",
        "    num_layer  = len(hidden_states)\n",
        "    batch_size, sequence_length, hidden_size = hidden_states[0].shape\n",
        "\n",
        "    last_token_pos = torch.sum(mask,dim=1) # Size([batch_size])\n",
        "\n",
        "    reference_logits = refernce_logits[torch.arange(len(mask)),last_token_pos -1 ,:] # Size([batch_size, vocab_size])\n",
        "    reference_logits_max_token = torch.argmax(reference_logits,dim = -1)\n",
        "\n",
        "    def suprisla(reference_logist, logist):\n",
        "        predicted_tokens = torch.argmax(logits, dim=-1)  # Shape: (batch_size,)\n",
        "        reference_probs = torch.nn.functional.softmax(reference_logits, dim=-1)\n",
        "        predicted_token_probs = reference_probs[torch.arange(batch_size), predicted_tokens]\n",
        "        surprisal = -torch.log(predicted_token_probs + 1e-9)\n",
        "\n",
        "        return torch.sum(surprisal)\n",
        "\n",
        "\n",
        "    for i , each_layer in enumerate(hidden_states):\n",
        "        layer_hidden = each_layer[torch.arange(batch_size),last_token_pos-1] # Size([batch_size, hidden_size]\n",
        "\n",
        "        #***************************************************************\n",
        "        # the only changes compared to logit_len\n",
        "        affine_map = affine_map_house[i]   #这里竟然没有报错，当我只有第一个layer的affine map的时候\n",
        "        layer_hidden_mapped = affine_map.predict(layer_hidden.cpu().numpy())\n",
        "        layer_hidden_mapped = torch.tensor(layer_hidden_mapped).to(device)\n",
        "\n",
        "\n",
        "\n",
        "        #***************************************************************\n",
        "        logits = torch.einsum('bd,vd->bv', layer_hidden_mapped, W_U)  # Size([batch_size, vocab_size])\n",
        "        logits_max_token = torch.argmax(logits,dim = -1)\n",
        "\n",
        "        identical_samples = torch.sum(logits_max_token == reference_logits_max_token)\n",
        "        each_layer_tuned_lens_Precision[i].append(identical_samples)\n",
        "        each_layer_tuned_lens_Surprisal[i].append(suprisla(reference_logits,logits))\n",
        "        # print(identical_samples)\n",
        "        # print(torch.sum(logits_max_token == reference_logits_max_token))\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "i = 0\n",
        "for batch in tqdm(DataLoader(validation_data, batch_size=batch_size, collate_fn=data_collator),total = math.ceil(2000/batch_size),desc = \"tuned_len_exp\"):\n",
        "    # print(batch)k\n",
        "    batch.to(device)\n",
        "    with torch.no_grad():\n",
        "        model_outputs = model(**batch,output_hidden_states=True)\n",
        "        hidden_states = model_outputs.hidden_states[1:]  # exclude the embedding layer\n",
        "        reference_logits = model_outputs.logits\n",
        "        mask = batch['attention_mask']\n",
        "\n",
        "    # reference_logits = reference_logits[torch.arange(len(mask)),torch.sum(mask,dim=1)] # Size([batch_size, vocab_size])\n",
        "    Compare_logit = tuned_len(hidden_states,mask,reference_logits)\n",
        "    i += 1\n",
        "    # print('one batch done')\n",
        "\n",
        "    # if i >= 5:\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_len_p = precision(each_layer_tuned_lens_Precision)\n",
        "tuned_len_p = surprisal(each_layer_tuned_lens_Surprisal)"
      ],
      "metadata": {
        "id": "enrkAKbmFB3H"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(tuned_len_p,f\"/content/drive/MyDrive/Patchscope-Reimplementation-main/Decoding_of_next_token_Prediction/tuned_len_P_{model_dict[model_num].split('/')[-1]}.pt\")\n",
        "torch.save(tuned_len_p,f\"/content/drive/MyDrive/Patchscope-Reimplementation-main/Decoding_of_next_token_Prediction/tuned_len_S_{model_dict[model_num].split('/')[-1]}.pt\")"
      ],
      "metadata": {
        "id": "Ns3myLOlPgsS"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_len_p"
      ],
      "metadata": {
        "id": "c8LDigboQBCd",
        "outputId": "d34b8a0a-f6e3-4726-81d0-2f79f9d003be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {0: tensor(0.1845, device='cuda:0'),\n",
              "             1: tensor(0.2020, device='cuda:0'),\n",
              "             2: tensor(0.2045, device='cuda:0'),\n",
              "             3: tensor(0.2205, device='cuda:0'),\n",
              "             4: tensor(0.2345, device='cuda:0'),\n",
              "             5: tensor(0.2345, device='cuda:0'),\n",
              "             6: tensor(0.2325, device='cuda:0'),\n",
              "             7: tensor(0.2395, device='cuda:0'),\n",
              "             8: tensor(0.2420, device='cuda:0'),\n",
              "             9: tensor(0.2450, device='cuda:0'),\n",
              "             10: tensor(0.2445, device='cuda:0'),\n",
              "             11: tensor(0.2585, device='cuda:0'),\n",
              "             12: tensor(0.2690, device='cuda:0'),\n",
              "             13: tensor(0.2875, device='cuda:0'),\n",
              "             14: tensor(0.3100, device='cuda:0'),\n",
              "             15: tensor(0.3270, device='cuda:0'),\n",
              "             16: tensor(0.4000, device='cuda:0'),\n",
              "             17: tensor(0.4505, device='cuda:0'),\n",
              "             18: tensor(0.4755, device='cuda:0'),\n",
              "             19: tensor(0.5220, device='cuda:0'),\n",
              "             20: tensor(0.5780, device='cuda:0'),\n",
              "             21: tensor(0.6505, device='cuda:0'),\n",
              "             22: tensor(0.7215, device='cuda:0'),\n",
              "             23: tensor(1., device='cuda:0')})"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_len_s"
      ],
      "metadata": {
        "id": "sC7ee37MFIl4",
        "outputId": "3cc3580a-9248-4635-e1a4-794ce83ef1b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {0: tensor(5.5387, device='cuda:0'),\n",
              "             1: tensor(5.2503, device='cuda:0'),\n",
              "             2: tensor(5.1506, device='cuda:0'),\n",
              "             3: tensor(4.9346, device='cuda:0'),\n",
              "             4: tensor(4.7680, device='cuda:0'),\n",
              "             5: tensor(4.7396, device='cuda:0'),\n",
              "             6: tensor(4.7775, device='cuda:0'),\n",
              "             7: tensor(4.7151, device='cuda:0'),\n",
              "             8: tensor(4.6886, device='cuda:0'),\n",
              "             9: tensor(4.6371, device='cuda:0'),\n",
              "             10: tensor(4.6395, device='cuda:0'),\n",
              "             11: tensor(4.4843, device='cuda:0'),\n",
              "             12: tensor(4.3771, device='cuda:0'),\n",
              "             13: tensor(4.2798, device='cuda:0'),\n",
              "             14: tensor(4.1091, device='cuda:0'),\n",
              "             15: tensor(3.9514, device='cuda:0'),\n",
              "             16: tensor(3.4422, device='cuda:0'),\n",
              "             17: tensor(3.0373, device='cuda:0'),\n",
              "             18: tensor(2.8325, device='cuda:0'),\n",
              "             19: tensor(2.5387, device='cuda:0'),\n",
              "             20: tensor(2.0637, device='cuda:0'),\n",
              "             21: tensor(1.6731, device='cuda:0'),\n",
              "             22: tensor(1.3910, device='cuda:0'),\n",
              "             23: tensor(1.1011, device='cuda:0')})"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "YPpmYblm_oo_",
        "outputId": "94f28fbf-bf6b-4d92-9d92-b445cb8533a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 896)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "X_train[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "ivG26GGG_oo_",
        "outputId": "0402c97b-cd91-4a3e-870e-d13823957a06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 896)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "Y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2aetG9r_opA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm9TWNGR_opA"
      },
      "source": [
        "we can conclude that tuned lens really outputperform vallina logit lean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOZH_xXc_opA"
      },
      "source": [
        "## 3.Token Identitiy Patchscope"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWeVupRt_opA"
      },
      "source": [
        "set the hidden state at the specic layer and position to the last"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho1qUnEf_opA"
      },
      "source": [
        "### A. Construct **Few-Shot Token Identity Prompts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "i7VSG8lS_opA"
      },
      "outputs": [],
      "source": [
        "# sample a random set of k tokens for all the models, where k was also randomly sampled from the interval [1, . . . , 10].\n",
        "\n",
        "# to construct 5 realization of token iDs series of variable length\n",
        "\n",
        "def craft_realization() -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    return: Size([num_realization, num_tokens])\n",
        "    \"\"\"\n",
        "\n",
        "    identity_prompts = []\n",
        "\n",
        "    num_realization = 5\n",
        "\n",
        "    for j in range(num_realization):\n",
        "        num_tokens = random.sample(range(tokenizer.vocab_size),random.randint(1,10))\n",
        "        prompt = ';'.join([ f\"{tokenizer.decode([i])} -> {tokenizer.decode([i])}\"  for i in num_tokens]) + '; ?'\n",
        "        identity_prompts.append(prompt)\n",
        "\n",
        "\n",
        "    return identity_prompts\n",
        "\n",
        "identity_token_prompts = craft_realization()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "C5diyhS6_opA",
        "outputId": "5424964e-ec94-41b6-ba98-1dbdf0e94ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['=( -> =(;_invoice -> _invoice; Record ->  Record; ?', '-bal -> -bal; booths ->  booths; ?', ' pomi ->  pomi; Кон ->  Кон; Xin ->  Xin;أكثر -> أكثر;StackNavigator -> StackNavigator;bucks -> bucks; Nov ->  Nov; ?', '럭 -> 럭;upgrade -> upgrade;.STRING -> .STRING; אין ->  אין; takes ->  takes; ?', ' Kund ->  Kund; relaciones ->  relaciones; dataGridViewTextBoxColumn ->  dataGridViewTextBoxColumn; defiant ->  defiant; ?']\n"
          ]
        }
      ],
      "source": [
        "print(identity_token_prompts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.tokenize(identity_token_prompts[0])"
      ],
      "metadata": {
        "id": "AZTgMNWMGutm"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "WPotkjCN_opA",
        "outputId": "1d3ed889-3301-4172-957c-70c228cc506c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModuleList(\n",
              "  (0-23): 24 x Qwen2DecoderLayer(\n",
              "    (self_attn): Qwen2Attention(\n",
              "      (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "      (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "      (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "      (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "    )\n",
              "    (mlp): Qwen2MLP(\n",
              "      (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "      (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "      (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "      (act_fn): SiLU()\n",
              "    )\n",
              "    (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "model.model.layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "B0cQazccNAXn",
        "outputId": "48e91c9e-2d17-4077-bd17-050038a723d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 896)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "xDpI5iay_opA"
      },
      "outputs": [],
      "source": [
        "debug_idx = 23\n",
        "def patch_hidden_state(source_hidden_states : tuple,batch_length:torch.Tensor, model=model,target_prompt = None) -> None:\n",
        "    \"\"\"\n",
        "    source_hidden_states: Tuple[Tensor:Size([batch_size, sequence_length, hidden_size])]: the hidden states of each layer of the model\n",
        "    batch_length: Size([batch_size])\n",
        "\n",
        "    target_prompt: str: the prompt the patched hidden states should be conditioned on\n",
        "\n",
        "    \"\"\"\n",
        "    patched_results = {l : [] for l in range(len(source_hidden_states))}\n",
        "\n",
        "\n",
        "    for layer_index, layer in enumerate(source_hidden_states):\n",
        "\n",
        "        layer = layer[torch.arange(len(batch_length)),batch_length -1 ,:] # Size([batch_size, hidden_size])\n",
        "\n",
        "\n",
        "        for i in range(layer.shape[0]):\n",
        "\n",
        "            hs_to_be_patched = layer[i]\n",
        "\n",
        "            # set a farward hook function\n",
        "            def patch_hidden_hook(module, input, output):\n",
        "                \"\"\"\n",
        "                input: Tuple[Tensor:Size([batch_size, sequence_length, hidden_size])]\n",
        "                output: Tuple[Size([batch_size, sequence_length, hidden_size])]: the hidden states of each layer of the model\n",
        "\n",
        "                \"\"\"\n",
        "                # print(f' ')\n",
        "                output[0][0,-1] = hs_to_be_patched\n",
        "\n",
        "                if layer_index == debug_idx:  # for debugging reason\n",
        "                    print(output[0][0,-1][:6])\n",
        "\n",
        "\n",
        "            # set a hook to the target layer\n",
        "            hook_handle = model.model.layers[layer_index].register_forward_hook(patch_hidden_hook)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output_Target = model(**tokenizer(target_prompt,return_tensors='pt').to(device),output_hidden_states=True)\n",
        "                target_logits = output_Target.logits[0][-1]\n",
        "                patched_results[layer_index].append(target_logits)\n",
        "\n",
        "            hook_handle.remove()\n",
        "\n",
        "\n",
        "\n",
        "            if layer_index == debug_idx:  # for debugging reason\n",
        "\n",
        "                print(output_Target.hidden_states[debug_idx + 1 ][0][-1][:6])\n",
        "\n",
        "\n",
        "    for k,v in patched_results.items():\n",
        "        patched_results[k] = torch.stack(v)\n",
        "\n",
        "    return patched_results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "gWj1PPSI_opA",
        "outputId": "195797ca-6ae7-4697-c5cb-d55c38e1e5ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "tokenizer(identity_token_prompts[0],return_tensors='pt',padding=True)['input_ids'].shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "pO5IsjT5_opA"
      },
      "outputs": [],
      "source": [
        "def cal_precision_1(source, target,sources_length):\n",
        "    \"\"\"\n",
        "    source: Size([batch_size, sequence_len, vocab_size])\n",
        "    target: Dict(K: num_layer, V: Size([batch_size, vocab_size]))\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    source = source[torch.arange(len(sources_length)),sources_length -1 ,:] # Size([batch_size, vocab_size])\n",
        "\n",
        "    for i in range(len(target)):\n",
        "        print(torch.sum(torch.argmax(source,dim=-1) == torch.argmax(target[i],dim=-1),dim = 0))\n",
        "\n",
        "def cal_surpisal(source, target,sources_length):\n",
        "    \"\"\"\n",
        "    source: Size([batch_size, sequence_len, vocab_size])\n",
        "    target: Dict(K: num_layer, V: Size([batch_size, vocab_size]))\n",
        "\n",
        "    \"\"\"\n",
        "    reference_probs = torch.nn.functional.softmax(source, dim=-1)\n",
        "\n",
        "    for i in range(len(target)):\n",
        "      predicted_tokens = torch.argmax(target[i], dim=-1)  # Shape: (batch_size,)\n",
        "\n",
        "      predicted_token_probs = reference_probs[torch.arange(len(sources_length)), predicted_tokens]\n",
        "      surprisal = -torch.log(predicted_token_probs + 1e-9)\n",
        "      print(torch.sum(surprisal))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "VwyCitwC_opA",
        "outputId": "1dc217ae-a191-496b-d8bf-cecc99c48764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-caf02e30c10c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#,total = (2000/ batch_size), desc = \"patchscope_exp\"):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# print(batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# run the first forward pass on the source prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;31m# into a HalfTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m             self.data = {\n\u001b[0m\u001b[1;32m    821\u001b[0m                 \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m             self.data = {\n\u001b[0;32m--> 821\u001b[0;31m                 \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             }\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "for idx, prompt in enumerate(identity_token_prompts):\n",
        "\n",
        "    # get the hidden satate of the prompt at the last positon\n",
        "    identity_token_P = [defaultdict(list)]\n",
        "    identity_token_S = [defaultdict(list)]\n",
        "\n",
        "    batch_size = 4\n",
        "    for batch in DataLoader(validation_data, batch_size=batch_size ,collate_fn=data_collator): #,total = (2000/ batch_size), desc = \"patchscope_exp\"):\n",
        "        # print(batch)\n",
        "        batch.to(device)\n",
        "\n",
        "        # run the first forward pass on the source prompt\n",
        "        with torch.no_grad():\n",
        "            # with autocast('cuda'):\n",
        "            # print(\"hello 123\")\n",
        "            model_outputs = model(**batch,output_hidden_states=True)\n",
        "            # print(\"heelo \")\n",
        "            hidden_states = model_outputs.hidden_states[1:]\n",
        "            reference_logits = model_outputs.logits # Size([batch_size, sequence_length, vocab_size])\n",
        "            batch_length = torch.sum(batch['attention_mask'],dim=1)\n",
        "\n",
        "        target_logtis = patch_hidden_state(hidden_states,batch_length,target_prompt=prompt)\n",
        "\n",
        "        # print(hidden_states[0][:,batch_length,:][:6])\n",
        "        cal_precision_1(reference_logits,target_logtis,batch_length)\n",
        "        cal_surpisal(reference_logits,target_logtis,batch_length)\n",
        "\n",
        "        # break\n",
        "\n",
        "\n",
        "        break\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SKpoEMw_opA"
      },
      "outputs": [],
      "source": [
        "# to check at the last layer if the logits are the same as the target logits"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}