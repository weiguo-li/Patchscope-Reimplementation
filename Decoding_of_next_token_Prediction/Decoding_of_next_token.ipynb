{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding of Next-Token Predictions\n",
    "\n",
    "This note book is designed to reimplement Pathscope **section 1:Next-Token Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device( 'cuda' if torch.cuda.is_available() else 'cup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.device_count())  # Should return the number of available GPUs\n",
    "print(torch.cuda.get_device_name(0))  # Should return the name of the GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,DataCollatorWithPadding\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# Your code here\n",
    "# import dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = [\"Qwen/Qwen2-0.5B\",\"Qwen/Qwen2.5-0.5B-Instruct\",\"Qwen/Qwen2-1.5B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_num = 0\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dict[model_num])\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dict[model_num]).to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(tokenizer('how are you doing today?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.models.qwen2.modeling_qwen2\n",
      "qwen2\n"
     ]
    }
   ],
   "source": [
    "print(model.__class__.__module__)\n",
    "print(model.config.model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.tokenization_qwen2_fast.Qwen2TokenizerFast"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2ForCausalLM\n",
    "model2 = Qwen2ForCausalLM.from_pretrained('Qwen/Qwen2-0.5B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get hidden states of each layer of the model\n",
    "\n",
    "def get_hidden_state(model, input_ids):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids.to(device), output_hidden_states=True)\n",
    "    return outputs.hidden_states[1:]  #Size([batch_size, num_layers, sequence_length, hidden_size])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b786462008a3457c9efc92019d9fcc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the Pile data \"https://huggingface.co/datasets/EleutherAI/pile\" from hugging face, unfortunately it is not available\n",
    "# use the following uncoprighted pile dataset instead\n",
    "# https://huggingface.co/datasets/monology/pile-uncopyrighted\n",
    "\n",
    "dataPile= load_dataset(\"monology/pile-uncopyrighted\",split='train',streaming=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset before spliting them \n",
    "# reference resource \"https://huggingface.co/docs/datasets/v3.3.2/stream\"\n",
    "\n",
    "dataPile = dataPile.shuffle(seed=50)\n",
    "training_data = dataPile.take(10000)\n",
    "# train : val = 5 : 1\n",
    "validation_data = dataPile.skip(10000).take(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'The present invention relates generally to the art of sputtering silicon-containing target materials in a reactive atmosphere, and more particularly to reactive sputtering of silicon targets further comprising a transition metal.\\nGerman Patent Application DE 3,906,453 Al to Szczybowski et al. discloses sputtering silicides, such as nickel silicide (NiSi.sub.2), in an oxidizing atmosphere to deposit dielectric oxide films.\\nThe present invention involves silicon-nickel alloys ranging from 3 to 18 weight percent nickel. Targets of silicon-nickel alloys are sputtered in an atmosphere comprising nitrogen, oxygen, inert gases and mixtures thereof to produce silicon-nickel containing coatings including oxides, nitrides and oxynitrides. The silicon-nickel compositions of the present invention comprise sufficient nickel to provide target stability and a desirable sputtering rate while keeping the absorption of the resulting films relatively low.',\n",
       "  'meta': {'pile_set_name': 'USPTO Backgrounds'}},\n",
       " {'text': '3. 3\\n\\n\\u200bThe streets are filled with people who want Hitler dead\\n\\n\\u200b\"who do you think you are , killing off the different people\"\\n\\n\\u200bI hear them cry, The SS are trying to calm them down but they are very unsuccessful. its quite funny to watch until that Molotov cocktail has just been throw. its really getting violent now. I should be down there, fight for my rights but I cant be. I\\'m stuck indoors, cursing my government for letting this idiot into power',\n",
       "  'meta': {'pile_set_name': 'Pile-CC'}},\n",
       " {'text': 'Desmond Boal & Ian Paisley: An End To The Affair\\n\\nThat Desmond Boal and Ian Paisley should die within six or seven months of each other seems somehow fitting. Paisley died last September and Boal last week, their exit from this world signalling not just the final act in a partnership that has left an indelible mark on Irish politics and history but the passing of an age, the like of which we will probably never see again nor soon even be able to imagine existed.\\n\\nDesmond Boal in an undated photograph\\n\\nIt was the most curious of collaborations, the gifted, cerebral, agnostic and very worldly barrister and the mesmeric, back street preacher cum demagogue united only by their aversion to the establishment forces that dominated their community. In the end the former’s principles could not find houseroom with the latter’s ambition and the alliance foundered. But in-between, in a few short years, they together made history.\\n\\nI was going to write a lengthy and hopefully erudite essay on the significance of the Boal-Paisley alliance but then I realised that myself and Andy Pollak had already done that, first in ‘Paisley’ which we co-wrote in 1986 and then in the update of the saga, ‘Paisley – From Demagogue to Democrat?’ which was written under my single byline in 2008.\\n\\nBoal celebrates his election to Stormont as the MP for Shankill. His constituency took in the Court Ward, which some regard as the birthplace of the Gusty Spence UVF. Boal’s more radical stance on social and economic issues reflected the area’s disregard for ‘fur-coat Unionism’, its traditional support for Independent Unionism and the subsequent development of the Progressive Unionist Party, the UVF’s political wing which embodied many of these attributes.\\n\\nSo here are three extracts from those books, the first two from the ‘Paisley’ book and the third from the second book. The first extract deals with the incident that brought the two men together for the first time; the second deals with the formation of the DUP and the gradual fracturing of the relationship between the two men and the last reflects Boal’s anger at Paisley in the wake of the power-sharing deal with Sinn Fein.\\n\\n(My own view of Boal’s final break with Paisley, for what it is worth, is that he objected not so much to the idea of breaking bread with the Provos, so to speak, but more to the fact that Paisley had gone back on his word never to have any truck with the IRA’s political wing, a commitment that had attracted tens of thousands of Loyalists to his cause, and some to prison cells and early graves. Boal had taken a similar line on Terence O’Neill’s meeting with the Dublin prime minister Sean Lemass. It was not so much the meeting he objected to – although he certainly was not happy about it – as that O’Neill had given Unionist MP’s at Stormont his word that he would never do such a thing.)\\n\\nBoal attempted to move Paisley away from the Free Presbyterian-dominated Protestant Unionist Party and broaden its base. He failed in that task but arguably Peter Robinson completed the job.\\n\\nThe first extract deals with an incident most readers of this blog have probably never heard of. But for a few weeks in the Summer of 1957, the town of Fethard-on-Sea in County Wexford was on lips throughout Ireland:\\n\\nThe second extract deals with the formation of the DUP, early signs of Paisley’s ambition to dominate anti-establishment Unionism, Boal’s failed efforts to make it a broad-based, social and economically radical Unionist alternative and his subsequent dalliances with politically dangerous ideas, dangerous to Paisley that is:\\n\\nFinally, post-St Andrews, came the final break between the two men and with it the end of an extraordinary era in Irish political life:\\n\\n2 responses to “Desmond Boal & Ian Paisley: An End To The Affair”\\n\\nBoal criticised Paisley for doing a deal with SF, yet it was Boal in January 1975 who came out publicly in favour of a federal Ireland. This seems never to be mentioned in assessments of Boal. Its been airbrushed out.',\n",
       "  'meta': {'pile_set_name': 'Pile-CC'}},\n",
       " {'text': '/*\\n *  R : A Computer Language for Statistical Data Analysis\\n *  file consolestructs.h\\n *  Copyright (C) 2008      The R Foundation\\n *\\n *  This program is free software; you can redistribute it and/or modify\\n *  it under the terms of the GNU General Public License as published by\\n *  the Free Software Foundation; either version 2 of the License, or\\n *  (at your option) any later version.\\n *\\n *  This program is distributed in the hope that it will be useful,\\n *  but WITHOUT ANY WARRANTY; without even the implied warranty of\\n *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n *  GNU General Public License for more details.\\n *\\n *  You should have received a copy of the GNU General Public License\\n *  along with this program; if not, a copy is available at\\n *  http://www.r-project.org/Licenses/\\n */\\n\\n\\n#ifndef _GUICOLORS_H_\\n#define\\t_GUICOLORS_H_\\n\\ntypedef enum {\\n  consolebg, consolefg, consoleuser, \\n  pagerbg, pagerfg, pagerhighlight,\\n  dataeditbg, dataeditfg, dataedituser,\\n  editorbg, editorfg,\\n  numGuiColors\\n} GuiColorIndex;\\n\\nextern rgb guiColors[numGuiColors];\\n\\n#endif\\n',\n",
       "  'meta': {'pile_set_name': 'Github'}}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(validation_data)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"1. Field of the Invention\\nThe present invention relates to toothbrushes and, in particular, to a toothbrush having a hollow handle defining a paste-holding cavity wherein toothpaste is forcibly dispensed therefrom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    16,     13,   8601,    315,    279,    758,   7459,    198,    785,\n",
       "           3042,  27130,  35616,    311,  25507,  36061,    288,    323,     11,\n",
       "            304,   3953,     11],\n",
       "        [ 49053,    438,   2940,   3193,  66713,   2940,    220, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([text,'hee asdf po afsdf '], return_tensors=\"pt\",max_length=21, truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "def preprocess(example):\n",
    "\n",
    "    text = example['text']\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=50, truncation=True) # dict_keys(['input_ids', 'attention_mask'])\n",
    "    inputs_ids = inputs['input_ids'][0] # becasue the batch size is 1 \n",
    "\n",
    "    if len(inputs_ids) > 5:\n",
    "        trim_len = random.randint(5,  len(inputs_ids) -1)   # to introduce randomness in the training data\n",
    "        inputs_ids = inputs_ids[:trim_len]\n",
    "\n",
    "    return {'input_ids': inputs_ids, 'attention_mask': inputs['attention_mask'][0][:len(inputs_ids)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.map(preprocess,remove_columns=['text','meta'])\n",
    "validation_data = validation_data.map(preprocess,remove_columns=['text','meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   35,   321,  1345,   496,   220,    16,    24,    24,    23,   271,\n",
       "         38704, 10099,   279,  2480,  5700, 52767,  1345,   496,  2860,  1283,\n",
       "           279,  4285, 12227]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design a collate function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can change the attentino mask to focus on the token of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2097152000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch\n",
    "torch.cuda.memory_summary()\n",
    "\n",
    "# Or more detailed\n",
    "torch.cuda.memory_allocated()\n",
    "torch.cuda.memory_reserved()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logit lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def logit_len_for_test(hidden_states,refernce_logits):\n",
    "    pass\n",
    "    \n",
    "\n",
    "\n",
    "def logit_len(hidden_states,mask,refernce_logits):\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_states: Tuple[Size([batch_size, sequence_length, hidden_size])]: the hidden states of each layer of the model\n",
    "\n",
    "    mask: Size([batch_size, sequence_length])\n",
    "\n",
    "    refernce_logits: Size([batch_size, sequence_length, vocab_size]): this will be converted to Size([batch_size, vocab_size])\n",
    "    \n",
    "    \"\"\"\n",
    "    W_U = model.lm_head.weight\n",
    "    \n",
    "    # using attention mask to get the last token of the sequence\n",
    "\n",
    "    num_layer  = len(hidden_states)\n",
    "    batch_size, sequence_length, hidden_size = hidden_states[0].shape\n",
    "\n",
    "    last_token_pos = torch.sum(mask,dim=1) # Size([batch_size])\n",
    "\n",
    "    reference_logits = refernce_logits[torch.arange(len(mask)),last_token_pos -1 ,:] # Size([batch_size, vocab_size])\n",
    "    reference_logits_max_token = torch.argmax(reference_logits,dim = -1)\n",
    "\n",
    "\n",
    "    for each_layer in hidden_states:\n",
    "        layer_hidden = each_layer[torch.arange(batch_size),last_token_pos-1] # Size([batch_size, hidden_size])\n",
    "        # layer_hidden = layer_hidden[torch.arange(batch_size),:] # Size([batch_size, hidden_size])\n",
    "        logits = torch.einsum('bd,vd->bv', layer_hidden, W_U)  # Size([batch_size, vocab_size])\n",
    "        logits_max_token = torch.argmax(logits,dim = -1)\n",
    "        print(torch.sum(logits_max_token == reference_logits_max_token))    \n",
    "\n",
    "\n",
    "    return  \n",
    "\n",
    "\n",
    "\n",
    "    # layer_indics = torch.arange(num_layer,device=hidden_states[0].device).view(num_layer,1)\n",
    "    # batch_indics = torch.arange(batch_size, device=hidden_states[0].device).view(1,batch_size)\n",
    "\n",
    "    # layer_indics = layer_indics.expand(num_layer,batch_size)\n",
    "    # batch_indics = batch_indics.expand(num_layer,batch_size)\n",
    "    # position = last_token_pos.view(1,batch_size).expand(num_layer,batch_size)\n",
    "\n",
    "    # result_hidden = hidden_states[layer_indics,batch_indics,position,:] # Size([layer, batch_size, hidden_size])\n",
    "\n",
    "    # logits = torch.einsum('bld,vd->blv', result_hidden, W_U)  # Size([layer, batch_size, vocab_size])\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # resutls_token_hidden = []\n",
    "    # for layer in hidden_states:\n",
    "    #     for i, b in enumerate(layer):\n",
    "    #         last_token_hidden = b[last_token_pos[i] - 1]\n",
    "    #         resutls_token_hidden.append(last_token_hidden)\n",
    "\n",
    "    # resutls_token_hidden = torch.tensor(resutls_token_hidden).reshape(len(hidden_states),len(mask),hidden_states[0].shape[-1]) # Size([layer, batch_size, hidden_size])\n",
    "    # logits = torch.einsum('bld,vd->blv', resutls_token_hidden, W_U)  # Size([layer, batch_size, vocab_size])\n",
    "\n",
    "    # return  torch.argmax(reference_logits,dim = -1) == torch.argmax(logits,dim=-1)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "i = 0\n",
    "for batch in DataLoader(validation_data, batch_size=batch_size, collate_fn=data_collator):\n",
    "    # print(batch)k\n",
    "    batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model(**batch,output_hidden_states=True)\n",
    "        hidden_states = model_outputs.hidden_states[1:]  # exclude the embedding layer\n",
    "        reference_logits = model_outputs.logits\n",
    "        mask = batch['attention_mask']\n",
    "\n",
    "    # reference_logits = reference_logits[torch.arange(len(mask)),torch.sum(mask,dim=1)] # Size([batch_size, vocab_size])\n",
    "    Compare_logit = logit_len(hidden_states,mask,reference_logits) \n",
    "    # break\n",
    "    i += 1\n",
    "    print('one batch done')\n",
    "\n",
    "    # if i >= 5:\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(# PyTorch\n",
    "torch.cuda.memory_summary())\n",
    "\n",
    "# Or more detailed\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric \n",
    "1. Precision@1\n",
    "2. Suprisal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_outputs['logits'].shape)\n",
    "print(model_outputs['hidden_states'][4].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_outputs['hidden_states'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tuned Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "config = model.config\n",
    "num_layers = config.num_hidden_layers\n",
    "\n",
    "X_train = {l : [] for l in range(num_layers)}\n",
    "Y_train = []  # finaly layer logits\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def get_last_token_hidden_states(hidden_states,mask) -> None:\n",
    "    \"\"\"\n",
    "    hiddenstates: Tuple[Size([batch_size, sequence_length, hidden_size])]: the hidden states of each layer of the model\n",
    "    \"\"\"\n",
    "\n",
    "    last_token_pos = torch.sum(mask,dim=1) # Size([batch_size])\n",
    "\n",
    "    for i, layer in enumerate(hidden_states):\n",
    "        X_train[i].append(layer[torch.arange(len(mask)),last_token_pos -1 ,:].cpu().numpy())   # Size([batch_size, hidden_size])\n",
    "\n",
    "    Y_train.append(hidden_states[-1][torch.arange(len(mask)),last_token_pos -1 ,:].cpu().numpy())   # Size([batch_size, hidden_size]))\n",
    "\n",
    "# to get the hidden states of each sequence\n",
    "for batch in DataLoader(training_data, batch_size=batch_size, collate_fn=data_collator):\n",
    "    batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model(**batch,output_hidden_states=True)\n",
    "    batch.to('cpu')\n",
    "    hidden_states = model_outputs.hidden_states[1:] # exclude the embedding layer\n",
    "    mask = batch['attention_mask'].cpu()\n",
    "    reference_logits = model_outputs.logits\n",
    "\n",
    "    get_last_token_hidden_states(hidden_states,mask)\n",
    "\n",
    "    del batch, model_outputs\n",
    "    torch.cuda.empty_cache()  # Clear GPU cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each layer,train a affing transformation to predict the final layer logits\n",
    "\n",
    "Y_train = np.concatenate(Y_train,axis=0) # Size([num_samples, hidden_size])\n",
    "\n",
    "affine_map_house = []\n",
    "\n",
    "for i in range(num_layers):\n",
    "    affine_map = LinearRegression(fit_intercept=True)\n",
    "    X_train[i] = np.concatenate(X_train[i],axis=0) # Size([num_samples, hidden_size])\n",
    "    affine_map.fit(X_train[i],Y_train)\n",
    "    affine_map_house.append(affine_map)\n",
    "    # break\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# do the same thing as logit_len but with the affine transformation bewteen the hidden states of each layer and the final layer logits\n",
    "\n",
    "\n",
    "def tuned_len(hidden_states,mask,refernce_logits,model = model):\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_states: Tuple[Size([batch_size, sequence_length, hidden_size])]: the hidden states of each layer of the model\n",
    "\n",
    "    mask: Size([batch_size, sequence_length])\n",
    "\n",
    "    refernce_logits: Size([batch_size, sequence_length, vocab_size]): this will be converted to Size([batch_size, vocab_size])\n",
    "    \n",
    "    \"\"\"\n",
    "    W_U = model.lm_head.weight\n",
    "    \n",
    "    # using attention mask to get the last token of the sequence\n",
    "\n",
    "    num_layer  = len(hidden_states)\n",
    "    batch_size, sequence_length, hidden_size = hidden_states[0].shape\n",
    "\n",
    "    last_token_pos = torch.sum(mask,dim=1) # Size([batch_size])\n",
    "\n",
    "    reference_logits = refernce_logits[torch.arange(len(mask)),last_token_pos -1 ,:] # Size([batch_size, vocab_size])\n",
    "    reference_logits_max_token = torch.argmax(reference_logits,dim = -1)\n",
    "\n",
    "\n",
    "    for i , each_layer in enumerate(hidden_states):\n",
    "        layer_hidden = each_layer[torch.arange(batch_size),last_token_pos-1] # Size([batch_size, hidden_size]\n",
    "\n",
    "        #***************************************************************\n",
    "        # the only changes compared to logit_len\n",
    "        affine_map = affine_map_house[i]   #这里竟然没有报错，当我只有第一个layer的affine map的时候\n",
    "        layer_hidden_mapped = affine_map.predict(layer_hidden.cpu().numpy())\n",
    "        layer_hidden_mapped = torch.tensor(layer_hidden_mapped).to(device)\n",
    "\n",
    "        #***************************************************************\n",
    "        logits = torch.einsum('bd,vd->bv', layer_hidden_mapped, W_U)  # Size([batch_size, vocab_size])\n",
    "        logits_max_token = torch.argmax(logits,dim = -1)\n",
    "        print(torch.sum(logits_max_token == reference_logits_max_token))    \n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "i = 0\n",
    "for batch in DataLoader(validation_data, batch_size=batch_size, collate_fn=data_collator):\n",
    "    # print(batch)k\n",
    "    batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model(**batch,output_hidden_states=True)\n",
    "        hidden_states = model_outputs.hidden_states[1:]  # exclude the embedding layer\n",
    "        reference_logits = model_outputs.logits\n",
    "        mask = batch['attention_mask']\n",
    "\n",
    "    # reference_logits = reference_logits[torch.arange(len(mask)),torch.sum(mask,dim=1)] # Size([batch_size, vocab_size])\n",
    "    Compare_logit = tuned_len(hidden_states,mask,reference_logits) \n",
    "    # break\n",
    "    i += 1\n",
    "    print('one batch done')\n",
    "\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can conclude that tuned lens really outputperform vallina logit lean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Token Identitiy Patchscope "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the hidden state at the specic layer and position to the last "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Construct **Few-Shot Token Identity Prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a random set of k tokens for all the models, where k was also randomly sampled from the interval [1, . . . , 10].\n",
    "\n",
    "# to construct 5 realization of token iDs series of variable length  \n",
    "\n",
    "\n",
    "\n",
    "def craft_realization() -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    return: Size([num_realization, num_tokens])\n",
    "    \"\"\"\n",
    "\n",
    "    identity_prompts = []\n",
    "\n",
    "    num_realization = 5\n",
    "    \n",
    "    for j in range(num_realization):\n",
    "        num_tokens = random.sample(range(tokenizer.vocab_size),random.randint(1,10))\n",
    "        prompt = ';'.join([ f\"{tokenizer.decode([i])} -> {tokenizer.decode([i])}\"  for i in num_tokens]) + '; ?'\n",
    "        identity_prompts.append(prompt)\n",
    "\n",
    "\n",
    "    return identity_prompts\n",
    "\n",
    "identity_token_prompts = craft_realization()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'identity_token_prompts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43midentity_token_prompts\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'identity_token_prompts' is not defined"
     ]
    }
   ],
   "source": [
    "print(identity_token_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-23): 24 x Qwen2DecoderLayer(\n",
       "    (self_attn): Qwen2Attention(\n",
       "      (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "      (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "      (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "      (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "    )\n",
       "    (mlp): Qwen2MLP(\n",
       "      (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "      (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "      (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_hidden_state(source_hidden_states : tuple,batch_length:torch.Tensor, model=model,target_prompt = None) -> None:\n",
    "    \"\"\"\n",
    "    source_hidden_states: Tuple[Tensor:Size([batch_size, sequence_length, hidden_size])]: the hidden states of each layer of the model\n",
    "    batch_length: Size([batch_size])\n",
    "\n",
    "    target_prompt: str: the prompt the patched hidden states should be conditioned on\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # # set a foward hook \n",
    "    # def patch_hidden_hook(module, input, output):\n",
    "    #     \"\"\"\n",
    "    #     output: Size([batch_size, sequence_length, hidden_size])\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     print(output)\n",
    "    #     # print(f' the shape of the output is  {output}')\n",
    "\n",
    "\n",
    "    patched_results = {l : [] for l in range(len(source_hidden_states))}\n",
    "\n",
    "\n",
    "    for layer_index, layer in enumerate(source_hidden_states):\n",
    "\n",
    "        layer = layer[torch.arange(len(batch_length)),batch_length -1 ,:] # Size([batch_size, hidden_size])\n",
    "\n",
    "\n",
    "        for i in range(layer.shape[0]):\n",
    "\n",
    "            hs_to_be_patched = layer[i]\n",
    "\n",
    "            # set a farward hook function\n",
    "            def patch_hidden_hook(module, input, output):\n",
    "                \"\"\"\n",
    "                input: Tuple[Tensor:Size([batch_size, sequence_length, hidden_size])]\n",
    "                output: Tuple[Size([batch_size, sequence_length, hidden_size])]: the hidden states of each layer of the model\n",
    "\n",
    "                \"\"\"\n",
    "                # print(f' ')\n",
    "                output[0][0,-1] = hs_to_be_patched\n",
    "\n",
    "                if layer_index == 23:  # for debugging reason \n",
    "                    print(output[0][0,-1])\n",
    "\n",
    "\n",
    "            # set a hook to the target layer\n",
    "            hook_handle = model.model.layers[layer_index].register_forward_hook(patch_hidden_hook)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_Target = model(**tokenizer(target_prompt,return_tensors='pt').to(device),output_hidden_states=True)\n",
    "                target_logits = output_Target.logits[0][-1]\n",
    "                patched_results[layer_index].append(target_logits)\n",
    "\n",
    "            hook_handle.remove()\n",
    "\n",
    "\n",
    "            if layer_index == 23:  # for debugging reason\n",
    "\n",
    "                print(output_Target.hidden_states[23 + 1][0][-1])\n",
    "\n",
    "    \n",
    "    for k,v in patched_results.items():\n",
    "        patched_results[k] = torch.stack(v)\n",
    "\n",
    "    return patched_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(identity_token_prompts[0],return_tensors='pt',padding=True)['input_ids'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_precision_1(source, target,sources_length):\n",
    "    \"\"\"\n",
    "    source: Size([batch_size, sequence_len, vocab_size])\n",
    "    target: Dict(K: num_layer, V: Size([batch_size, vocab_size]))\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    source = source[torch.arange(len(sources_length)),sources_length -1 ,:] # Size([batch_size, vocab_size])\n",
    "\n",
    "    for i in range(len(target)):\n",
    "        print(torch.sum(torch.argmax(source,dim=-1) == torch.argmax(target[i],dim=-1),dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello 123\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 15.06 MiB is free. Including non-PyTorch memory, this process has 10.88 GiB memory in use. Of the allocated memory 10.22 GiB is allocated by PyTorch, and 515.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# with autocast('cuda'):\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello 123\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheelo \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m model_outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/huggingface/transformers/src/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/huggingface/transformers/src/transformers/models/qwen2/modeling_qwen2.py:873\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[1;32m    872\u001b[0m slice_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m-\u001b[39mlogits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[0;32m--> 873\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 15.06 MiB is free. Including non-PyTorch memory, this process has 10.88 GiB memory in use. Of the allocated memory 10.22 GiB is allocated by PyTorch, and 515.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for prompt in identity_token_prompts:\n",
    "\n",
    "    # get the hidden satate of the prompt at the last positon \n",
    "\n",
    "\n",
    "    batch_size = 4\n",
    "    for batch in DataLoader(validation_data, batch_size=batch_size, collate_fn=data_collator):\n",
    "        # print(batch)\n",
    "        batch.to(device)\n",
    "\n",
    "        # run the first forward pass on the source prompt\n",
    "        with torch.no_grad():\n",
    "            # with autocast('cuda'):\n",
    "            print(\"hello 123\")\n",
    "            model_outputs = model(**batch,output_hidden_states=True)\n",
    "            print(\"heelo \")\n",
    "            hidden_states = model_outputs.hidden_states[1:]\n",
    "            reference_logits = model_outputs.logits # Size([batch_size, sequence_length, vocab_size])\n",
    "            batch_length = torch.sum(batch['attention_mask'],dim=1)\n",
    "\n",
    "        target_logtis = patch_hidden_state(hidden_states,batch_length,target_prompt=prompt) \n",
    "\n",
    "\n",
    "        \n",
    "        cal_precision_1(reference_logits,target_logtis,batch_length)\n",
    "\n",
    "        batch.to('cpu')\n",
    "        del batch, model_outputs\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "        break\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 28 00:12:10 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti     Off |   00000000:3B:00.0 Off |                  N/A |\n",
      "| 25%   45C    P5             15W /  250W |    8144MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   2241573      C   .../students/wli/miniconda3/bin/python       8140MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (483644964.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[107], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    for\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# to check at the last layer if the logits are the same as the target logits "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
